{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6093cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/network/.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: pandas in /home/network/.local/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /home/network/.local/lib/python3.10/site-packages (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/network/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/network/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/network/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/network/.local/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/network/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/network/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/network/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/network/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/network/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/network/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/network/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/network/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/network/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/network/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/network/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/network/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn torch pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cb0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./awgn_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9b5e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1153"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_columns = len(df.columns)\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc96c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053d3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "class ErrorCorrectionDataset:\n",
    "    \"\"\"\n",
    "    Dataset for AWGN error correction.\n",
    "    Each sample contains:\n",
    "      - X: corrupted bytes as a sequence [seq_len, 1], normalized to [-1, 1]\n",
    "      - y: original bytes as class indices [seq_len]\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, psnr_pools: List[float], train_size: float = 0.8, seq_len=576, normalize=True):\n",
    "        self.df = df\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.psnr_pools = psnr_pools\n",
    "        self.train_size = train_size\n",
    "        self.seq_len = seq_len\n",
    "        self.normalize = normalize\n",
    "        self.corrupted_cols = [f\"Corrupted_{i}\" for i in range(seq_len)]\n",
    "        self.original_cols = [f\"Original_{i}\" for i in range(seq_len)]\n",
    "        self.X = dataframe[[f\"Corrupted_{i}\" for i in range(seq_len)]].values.astype(np.uint8)\n",
    "        self.Y = dataframe[[f\"Original_{i}\" for i in range(seq_len)]].values.astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        corrupted = self.X[idx]  # shape (576,)\n",
    "        original = self.Y[idx]   # shape (576,)\n",
    "\n",
    "        if self.normalize:\n",
    "            corrupted = (corrupted.astype(np.float32) / 127.5) - 1.0\n",
    "        else:\n",
    "            corrupted = corrupted.astype(np.float32)\n",
    "\n",
    "        X = torch.tensor(corrupted, dtype=torch.float32).unsqueeze(-1)  # [576, 1]\n",
    "        y = torch.tensor(original, dtype=torch.long)                    # [576]\n",
    "        return X, y\n",
    "\n",
    "    def _split_tensor(self, sub_df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Helper to split a dataframe into train/test torch tensors.\"\"\"\n",
    "        n_train = int(self.train_size * len(sub_df))\n",
    "        sub_df = sub_df.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "\n",
    "        X = sub_df[self.corrupted_cols].values.astype(\"float32\") / 255.0\n",
    "        y = sub_df[self.original_cols].values.astype(\"float32\") / 255.0\n",
    "\n",
    "        X_train, X_test = X[:n_train], X[n_train:]\n",
    "        y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(X_train, dtype=torch.float32).to(self.device),\n",
    "            torch.tensor(y_train, dtype=torch.float32).to(self.device),\n",
    "            torch.tensor(X_test, dtype=torch.float32).to(self.device),\n",
    "            torch.tensor(y_test, dtype=torch.float32).to(self.device),\n",
    "        )\n",
    "\n",
    "    def prepare_datasets(self) -> List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Returns a list of datasets (X_train, y_train, X_test, y_test) for each PSNR pool.\n",
    "        \"\"\"\n",
    "        datasets = []\n",
    "        for psnr in self.psnr_pools:\n",
    "            sub_df = self.df[self.df[\"PSNR\"] == psnr]\n",
    "            datasets.append(self._split_tensor(sub_df))\n",
    "        return datasets\n",
    "\n",
    "    def generalized_dataset(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a single dataset (X_train, y_train, X_test, y_test) combining all PSNR pools.\n",
    "        \"\"\"\n",
    "        return self._split_tensor(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f17fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_pools = [0.025, 0.1, 1, 2, 5, 50]\n",
    "psnr_pools.reverse()\n",
    "Dataset = ErrorCorrectionDataset(df, psnr_pools, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98d2114",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ErrorCorrectionDataset' object has no attribute 'corrupted_cols'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m error_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m generalized_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mgeneralized_dataset()\n",
      "Cell \u001b[0;32mIn[6], line 63\u001b[0m, in \u001b[0;36mErrorCorrectionDataset.prepare_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m psnr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpsnr_pools:\n\u001b[1;32m     62\u001b[0m     sub_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPSNR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m psnr]\n\u001b[0;32m---> 63\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_df\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m, in \u001b[0;36mErrorCorrectionDataset._split_tensor\u001b[0;34m(self, sub_df)\u001b[0m\n\u001b[1;32m     40\u001b[0m n_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sub_df))\n\u001b[1;32m     41\u001b[0m sub_df \u001b[38;5;241m=\u001b[39m sub_df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 43\u001b[0m X \u001b[38;5;241m=\u001b[39m sub_df[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrupted_cols\u001b[49m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     44\u001b[0m y \u001b[38;5;241m=\u001b[39m sub_df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_cols]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     46\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m X[:n_train], X[n_train:]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ErrorCorrectionDataset' object has no attribute 'corrupted_cols'"
     ]
    }
   ],
   "source": [
    "error_datasets = Dataset.prepare_datasets()\n",
    "generalized_dataset = Dataset.generalized_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8145eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWGNErrorCorrector(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based error correction model.\n",
    "    Takes in corrupted byte sequence and predicts original bytes.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=1, hidden_size=256, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.out_fc = nn.Linear(hidden_size, 256)  # 256-way classification for byte values\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, 1]\n",
    "        x_proj = torch.relu(self.input_fc(x))   # [batch, seq_len, hidden]\n",
    "        out, _ = self.lstm(x_proj)              # [batch, seq_len, hidden]\n",
    "        logits = self.out_fc(out)               # [batch, seq_len, 256]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dee6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "n_features = num_columns // 2\n",
    "model = AWGNErrorCorrector(n_features).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb03111",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (3968) to match target batch_size (2285568).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_train_00)  \n\u001b[0;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_00\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m        \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (3968) to match target batch_size (2285568)."
     ]
    }
   ],
   "source": [
    "X_train_00 = error_datasets[0][0].unsqueeze(0)\n",
    "y_train_00 = error_datasets[0][1].unsqueeze(0)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_00)  \n",
    "    loss = criterion(\n",
    "        outputs.reshape(-1, 256),     \n",
    "        y_train_00.reshape(-1)        \n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008e0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3968, 576]), torch.Size([1, 3968, 576]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_00.shape, X_train_00.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23818b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=0.659095\n",
      "Epoch 2, Loss=0.659102\n",
      "Epoch 3, Loss=0.658696\n",
      "Epoch 4, Loss=0.658377\n",
      "Epoch 5, Loss=0.658324\n",
      "Epoch 6, Loss=0.657725\n",
      "Epoch 7, Loss=0.657372\n",
      "Epoch 8, Loss=0.657420\n",
      "Epoch 9, Loss=0.656842\n",
      "Epoch 10, Loss=0.656712\n",
      "Epoch 11, Loss=0.656581\n",
      "Epoch 12, Loss=0.656127\n",
      "Epoch 13, Loss=0.656123\n",
      "Epoch 14, Loss=0.655745\n",
      "Epoch 15, Loss=0.655605\n",
      "Epoch 16, Loss=0.655392\n",
      "Epoch 17, Loss=0.655119\n",
      "Epoch 18, Loss=0.655005\n",
      "Epoch 19, Loss=0.654697\n",
      "Epoch 20, Loss=0.654590\n",
      "Epoch 21, Loss=0.654306\n",
      "Epoch 22, Loss=0.654172\n",
      "Epoch 23, Loss=0.653945\n",
      "Epoch 24, Loss=0.653755\n",
      "Epoch 25, Loss=0.653577\n",
      "Epoch 26, Loss=0.653323\n",
      "Epoch 27, Loss=0.653186\n",
      "Epoch 28, Loss=0.652943\n",
      "Epoch 29, Loss=0.652797\n",
      "Epoch 30, Loss=0.652611\n",
      "Epoch 31, Loss=0.652422\n",
      "Epoch 32, Loss=0.652240\n",
      "Epoch 33, Loss=0.652032\n",
      "Epoch 34, Loss=0.651858\n",
      "Epoch 35, Loss=0.651666\n",
      "Epoch 36, Loss=0.651510\n",
      "Epoch 37, Loss=0.651346\n",
      "Epoch 38, Loss=0.651180\n",
      "Epoch 39, Loss=0.651041\n",
      "Epoch 40, Loss=0.650888\n",
      "Epoch 41, Loss=0.650742\n",
      "Epoch 42, Loss=0.650579\n",
      "Epoch 43, Loss=0.650387\n",
      "Epoch 44, Loss=0.650193\n",
      "Epoch 45, Loss=0.649996\n",
      "Epoch 46, Loss=0.649831\n",
      "Epoch 47, Loss=0.649713\n",
      "Epoch 48, Loss=0.649615\n",
      "Epoch 49, Loss=0.649500\n",
      "Epoch 50, Loss=0.649320\n",
      "Epoch 51, Loss=0.649073\n",
      "Epoch 52, Loss=0.648828\n",
      "Epoch 53, Loss=0.648662\n",
      "Epoch 54, Loss=0.648580\n",
      "Epoch 55, Loss=0.648493\n",
      "Epoch 56, Loss=0.648322\n",
      "Epoch 57, Loss=0.648091\n",
      "Epoch 58, Loss=0.647890\n",
      "Epoch 59, Loss=0.647768\n",
      "Epoch 60, Loss=0.647664\n"
     ]
    }
   ],
   "source": [
    "X_train_01 = error_datasets[1][0].unsqueeze(0)\n",
    "y_train_01 = error_datasets[1][1].unsqueeze(0)\n",
    "\n",
    "for epoch in range(80):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_01)  \n",
    "    loss = criterion(\n",
    "        outputs.view(-1, 256),     \n",
    "        y_train_01.view(-1)        \n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=0.677091\n",
      "Epoch 2, Loss=0.677451\n",
      "Epoch 3, Loss=0.677787\n",
      "Epoch 4, Loss=0.677483\n",
      "Epoch 5, Loss=0.676791\n",
      "Epoch 6, Loss=0.676596\n",
      "Epoch 7, Loss=0.676641\n",
      "Epoch 8, Loss=0.676182\n",
      "Epoch 9, Loss=0.675914\n",
      "Epoch 10, Loss=0.675907\n",
      "Epoch 11, Loss=0.675466\n",
      "Epoch 12, Loss=0.675289\n",
      "Epoch 13, Loss=0.675119\n",
      "Epoch 14, Loss=0.674769\n",
      "Epoch 15, Loss=0.674645\n",
      "Epoch 16, Loss=0.674327\n",
      "Epoch 17, Loss=0.674165\n",
      "Epoch 18, Loss=0.673893\n",
      "Epoch 19, Loss=0.673706\n",
      "Epoch 20, Loss=0.673477\n"
     ]
    }
   ],
   "source": [
    "X_train_02 = error_datasets[2][0].unsqueeze(0)\n",
    "y_train_02 = error_datasets[2][1].unsqueeze(0)\n",
    "\n",
    "for epoch in range(60):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_02)  \n",
    "    loss = criterion(\n",
    "        outputs.view(-1, 256),     \n",
    "        y_train_02.view(-1)        \n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef8879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=0.690970\n",
      "Epoch 2, Loss=0.690746\n",
      "Epoch 3, Loss=0.690396\n",
      "Epoch 4, Loss=0.690054\n",
      "Epoch 5, Loss=0.689600\n",
      "Epoch 6, Loss=0.689132\n",
      "Epoch 7, Loss=0.688616\n",
      "Epoch 8, Loss=0.688094\n",
      "Epoch 9, Loss=0.687588\n",
      "Epoch 10, Loss=0.687144\n"
     ]
    }
   ],
   "source": [
    "X_train_03 = error_datasets[3][0].unsqueeze(0)\n",
    "y_train_03 = error_datasets[3][1].unsqueeze(0)\n",
    "\n",
    "for epoch in range(40):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_03)  \n",
    "    loss = criterion(\n",
    "        outputs.view(-1, 256),     \n",
    "        y_train_03.view(-1)        \n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_04 = error_datasets[4][0].unsqueeze(0)\n",
    "y_train_04 = error_datasets[4][1].unsqueeze(0)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_04)  \n",
    "    loss = criterion(\n",
    "        outputs.view(-1, 256),     \n",
    "        y_train_04.view(-1)        \n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0073224",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_05 = error_datasets[5][0].unsqueeze(0)\n",
    "y_train_05 = error_datasets[5][1].unsqueeze(0)\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_05)  \n",
    "    loss = criterion(\n",
    "        outputs.view(-1, 256),     \n",
    "        y_train_05.view(-1)        \n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss=0.664328\n",
      "Epoch 2, Loss=0.664662\n",
      "Epoch 3, Loss=0.664613\n",
      "Epoch 4, Loss=0.664188\n",
      "Epoch 5, Loss=0.663560\n",
      "Epoch 6, Loss=0.662773\n",
      "Epoch 7, Loss=0.661944\n",
      "Epoch 8, Loss=0.661126\n",
      "Epoch 9, Loss=0.660486\n",
      "Epoch 10, Loss=0.660104\n",
      "Epoch 11, Loss=0.659872\n",
      "Epoch 12, Loss=0.659651\n",
      "Epoch 13, Loss=0.659421\n",
      "Epoch 14, Loss=0.659322\n",
      "Epoch 15, Loss=0.659249\n",
      "Epoch 16, Loss=0.659121\n",
      "Epoch 17, Loss=0.659037\n",
      "Epoch 18, Loss=0.659040\n",
      "Epoch 19, Loss=0.659243\n",
      "Epoch 20, Loss=0.660288\n"
     ]
    }
   ],
   "source": [
    "X_train = generalized_dataset[0].unsqueeze(0)\n",
    "y_train = generalized_dataset[1].unsqueeze(0)\n",
    "\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)  \n",
    "    loss = criterion(\n",
    "        outputs.view(-1, 256),     \n",
    "        y_train.view(-1)        \n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss={loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdbf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  Loss = 0.662494, Test Byte Accuracy = 0.500357%\n"
     ]
    }
   ],
   "source": [
    "X_test = generalized_dataset[2].unsqueeze(0)\n",
    "y_test = generalized_dataset[3].unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # ---- Test metrics ----\n",
    "    test_outputs = model(X_test)  \n",
    "    test_loss = criterion(\n",
    "        test_outputs.view(-1, 256),     \n",
    "        y_test.view(-1)        \n",
    "    )\n",
    "    \n",
    "    test_preds = test_outputs.argmax(dim=-1)\n",
    "    test_targets = y_test\n",
    "    test_acc = (test_preds == test_targets).float().mean().item()\n",
    "\n",
    "    print(f\"Test  Loss = {test_loss.item():.6f}, Test Byte Accuracy = {test_acc*100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b13d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.662961, Train Byte Accuracy = 0.513877%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # ---- Train metrics ----\n",
    "    train_outputs = model(X_train)\n",
    "    train_loss = criterion(\n",
    "        train_outputs.view(-1, 256),     \n",
    "        y_train.view(-1)        \n",
    "    )\n",
    "\n",
    "    train_preds = train_outputs.argmax(dim=-1)\n",
    "    train_targets = y_train\n",
    "    train_acc = (train_preds == train_targets).float().mean().item()\n",
    "\n",
    "    print(f\"Train Loss = {train_loss.item():.6f}, Train Byte Accuracy = {train_acc*100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f91045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./error_correction/error_corrector_updated_dataset.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_path = \"./awgn_error_corrector.pth\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
